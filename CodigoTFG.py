# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u_gnSYA9T267oI6_8pn8Wk38KG5Uvj9-
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd

df = pd.read_excel('TFGtemplate.xlsx')
df.head()
 # Para ver las primeras filas del dataset #

"""PREPROCESADO DE DATOS"""

#######Agrupar por Partido y Año y combinar el texto######
df_agrupado = df.groupby(['Partido', 'Año'], as_index=False).agg({
    'Texto Castellano': ' '.join,                      # Une el texto
    'Longitud palabras': 'sum',                        # Suma la longitud
    'Ideología': 'first',                              # Toma la ideología (asumimos que es la misma)
    'Escaños Obtenidos': 'first',                      # Igual para escaños
    '% Escaños obtenidos': 'first',
    'Páginas Totales': 'first',
    'Nº caracteres/página': 'sum'                        # Suma la longitud
})

##### Reiniciar ID si lo necesitas ######
df_agrupado.insert(0, 'ID', range(1, len(df_agrupado) + 1))

##### Mostrar primeras filas ######
print(df_agrupado.head())

##### Mostrar las primeras 20 filas del DataFrame agrupado ####
print(df_agrupado.head(15))
##### Podemos ver que en el de ciudadanos se ha hecho correctamente #####

#### Procedemos ahora a listar las stopwords ####
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')

stop_words = stopwords.words('spanish')
extra = ['informacion', 'él']
stop_words.extend(extra)
stop_words = set(stop_words)

print(sorted(stop_words)[:100])

def eliminar_stopwords(texto):
    palabras = texto.split()  # Dividir el texto en palabras
    palabras_filtradas = [palabra for palabra in palabras if palabra.lower() not in stop_words]
    return ' '.join(palabras_filtradas)

df_agrupado['Texto sin stopwords'] = df_agrupado['Texto Castellano'].apply(eliminar_stopwords)

### compruebo que todo está bien ###
print(df_agrupado.head(20))

#### Procedemos ahora a eliminar signos de puntuación ####
import re

def eliminar_puntuacion(texto):
    # 1) Cada signo de puntuación (todo lo que no sea letra/dígito/espacio)
    texto = re.sub(r'[^\w\s]', ' ', texto)
    # 2) Colapsa múltiples espacios en uno solo
    texto = re.sub(r'\s+', ' ', texto)
    return texto.strip()

df_agrupado['Texto limpio'] = (
    df_agrupado['Texto sin stopwords']
      .apply(eliminar_puntuacion)
)

### compruebo que todo está bien ###
print(df_agrupado.head(4))

#### Convertimos ahora todo a minúsculas ####
def convertir_a_minusculas(texto):
    return texto.lower()

df_agrupado['Texto limpio'] = df_agrupado['Texto limpio'].apply(convertir_a_minusculas)

### compruebo que todo está bien ###
print(df_agrupado.head(4))

#### Procedemos a eliminar espacios extras ####
def eliminar_espacios_extra(texto):
    return ' '.join(texto.split())

df_agrupado['Texto limpio'] = df_agrupado['Texto limpio'].apply(eliminar_espacios_extra)

#### Procedemos a eliminar números ####
def eliminar_numeros(texto):
    # devuelve el texto sin ningún dígito
    return ''.join([i for i in texto if not i.isdigit()])

# Aplicamos la función a toda la columna
df_agrupado['Texto limpio'] = df_agrupado['Texto limpio'].apply(eliminar_numeros)

### compruebo que todo está bien ###
print(df_agrupado.head(4))

!pip install unidecode
from unidecode import unidecode

# Función para eliminar acentos
def eliminar_acentos(texto):
    return unidecode(texto)

# Aplicar la eliminación de acentos a los textos
df_agrupado['Texto sin acentos'] = df_agrupado['Texto limpio'].apply(eliminar_acentos)

# Verificar que todo está bien
print(df_agrupado.head(15))

!pip install -U spacy
!python -m spacy download es_core_news_sm

import spacy
nlp = spacy.load('es_core_news_sm')

#### Procedemos ahora a la lematización del texto ####

def lematizar_texto(texto):
    doc = nlp(texto)
    lemas = []
    for token in doc:
        # 1) Filtrar stopwords, puntuación, pronombres y tokens muy cortos
        if token.is_stop or token.is_punct or token.pos_ == 'PRON' or len(token.text) <= 1:
            continue

        # 2) Si es verbo, mantenemos la forma original
        if token.pos_ == 'VERB':
            lemas.append(token.text)
        # 3) Si es sustantivo o adjetivo, usamos el lema (singular, masculino)
        elif token.pos_ in {'NOUN', 'ADJ'}:
            lemas.append(token.lemma_)
        # 4) Para el resto (ADV, etc.), también usamos el lema
        else:
            lemas.append(token.lemma_)
    return ' '.join(lemas)

# Aplicar sobre la columna limpia y sin acentos
df_agrupado['Texto lematizado'] = (
    df_agrupado['Texto sin acentos']
      .apply(lematizar_texto)
)

# Verificar ejemplos concretos
print(df_agrupado.loc[:5, ['Texto sin acentos','Texto lematizado']])

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from wordcloud import WordCloud, STOPWORDS

# 1) Crear un set de stopwords que incluya 'él' y 'el'
mis_stopwords = STOPWORDS.union({'él','el'})

# 2) Unir todo el texto lematizado
texto_lemas = ' '.join(df_agrupado['Texto lematizado'])

# Convertir mis_stopwords a lista
mis_stopwords_list = list(mis_stopwords)

# Preparar la lista de documentos
docs = df_agrupado['Texto lematizado'].dropna().tolist()

# 3) Contamos solo unigramas con CountVectorizer
cv_uni = CountVectorizer(
    ngram_range=(1, 1),          # unigramas únicamente
    stop_words=mis_stopwords_list,
    min_df=1
)
X_uni = cv_uni.fit_transform(docs)

# 4) Extraer tokens y sus recuentos
terms = cv_uni.get_feature_names_out()      # array de palabras
counts = X_uni.sum(axis=0).A1               # array de frecuencias

# 5) Formar el diccionario {palabra: recuento}
freq_dict = dict(zip(terms, counts))

# 6) Crear WordCloud a partir de esas frecuencias
wordcloud = WordCloud(
    width=3000,
    height=2000,
    background_color='white',
    colormap='Set2',
    prefer_horizontal=1.0,
).generate_from_frequencies(freq_dict)

# 7) Dibujar Word Cloud
plt.figure(figsize=(10,7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.tight_layout()
plt.show()

"""ANÁLISIS BIGRAMAS Y TRIGRAMAS"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from unidecode import unidecode
import unicodedata


# 1) Construye la lista de documentos (texto lematizado) sin NaN
dft = df_agrupado['Texto lematizado'].dropna().tolist()

# 2) Convierte el set de stopwords a lista
mis_stopwords_list = list(mis_stopwords)

# 3) Inicializa CountVectorizer para unigramas, bigramas y trigramas
cv = CountVectorizer(
    ngram_range=(1, 3),
    stop_words=mis_stopwords_list,  # ahora es una lista, no un set
    min_df=1
)

# 4) Ajusta y transforma
X_counts = cv.fit_transform(dft)

# 5) Extrae términos y recuentos
terms = cv.get_feature_names_out()
counts = X_counts.sum(axis=0).A1

# 6) Crea DataFrame y ordena
df_counts = pd.DataFrame({
    'N-Grama': terms,
    'Recuento': counts
}).sort_values('Recuento', ascending=False)

# 7) Top 30 y gráfica
df_top30 = df_counts.head(30)
plt.figure(figsize=(15, 7))
plt.bar(df_top30['N-Grama'], df_top30['Recuento'])
plt.xticks(rotation=90)
plt.ylabel('Recuento')
plt.title('Top 30 Unigramas')
plt.tight_layout()
plt.show()

# Mostrar los 30 unigramas más frecuentes junto a su recuento
print(df_top30)

# 1) Construye la lista de documentos (texto lematizado) sin NaN
docs = df_agrupado['Texto lematizado'].dropna().tolist()

# 2) Convierte el set de stopwords a lista
mis_stopwords_list = list(mis_stopwords)

# 3) Configura CountVectorizer para **solo bigramas**, eliminando acentos justo antes:
cv_bi = CountVectorizer(
    ngram_range=(2, 2),            # solo bigramas
    stop_words=mis_stopwords_list, # lista de stopwords personalizada
    min_df=1,                      # mínimo de documentos donde debe aparecer
    preprocessor=lambda doc: unidecode(doc)  # elimina acentos antes de tokenizar
)

# 4) Ajusta y transforma el corpus
X_bi = cv_bi.fit_transform(docs)

# 5) Extrae bigramas y recuentos totales
bigrams = cv_bi.get_feature_names_out()
counts_bi = X_bi.sum(axis=0).A1

# 6) Crea DataFrame ordenado por recuento
df_bigrams = pd.DataFrame({
    'Bigrama': bigrams,
    'Recuento': counts_bi
}).sort_values('Recuento', ascending=False)

# 7) Selecciona los 30 más frecuentes
df_top30_bi = df_bigrams.head(30)

# 8) Graficar
plt.figure(figsize=(15, 7))
plt.bar(df_top30_bi['Bigrama'], df_top30_bi['Recuento'])
plt.xticks(rotation=90)
plt.ylabel('Recuento')
plt.title('Top 30 Bigramas')
plt.tight_layout()
plt.show()

# Mostrar el top 30 con sus recuentos
print(df_top30_bi)

# 1) Construye la lista de documentos (texto lematizado) sin NaN
docs = df_agrupado['Texto lematizado'].dropna().tolist()

# 2) Convierte el set de stopwords a lista
mis_stopwords_list = list(mis_stopwords)

# 3) Configura CountVectorizer para **solo trigramas**, eliminando acentos antes:
cv_tri = CountVectorizer(
    ngram_range=(3, 3),            # sólo trigramas
    stop_words=mis_stopwords_list, # lista de stopwords personalizada
    min_df=1,                      # mínimo de documentos donde debe aparecer
    preprocessor=lambda doc: unidecode(doc)  # elimina acentos antes de tokenizar
)

# 4) Ajusta y transforma el corpus
X_tri = cv_tri.fit_transform(docs)

# 5) Extrae trigramas y sus recuentos totales
trigrams = cv_tri.get_feature_names_out()
counts_tri = X_tri.sum(axis=0).A1

# 6) Crea DataFrame ordenado por recuento
df_trigrams = pd.DataFrame({
    'Trigrama': trigrams,
    'Recuento': counts_tri
}).sort_values('Recuento', ascending=False)

# 7) Selecciona los 30 más frecuentes
df_top30_tri = df_trigrams.head(30)

# 8) Graficar
plt.figure(figsize=(15, 7))
plt.bar(df_top30_tri['Trigrama'], df_top30_tri['Recuento'])
plt.xticks(rotation=90)
plt.ylabel('Recuento')
plt.title('Top 30 Trigramas')
plt.tight_layout()
plt.show()

# Mostrar el top 30 con sus recuentos
print(df_top30_tri)

"""ANÁLISIS BIGRAMAS Y TRIGRAMAS : COMPARATIVA PP-PSOE"""

# 1) Filtrar solo los programas del PP
df_pp = df_agrupado[df_agrupado['Partido'] == 'PP']

# 2) Sacar la columna lematizada y eliminar posibles NaN
texts_pp = df_pp['Texto lematizado'].dropna().tolist()

# 3) Convertir el set de stopwords a lista
mis_stopwords_list = list(mis_stopwords)

# 4) Configurar CountVectorizer para solo bigramas sin acentos
vec_pp = CountVectorizer(
    ngram_range=(2, 2),                      # solo bigramas
    stop_words=mis_stopwords_list,           # lista de stopwords personalizada
    preprocessor=lambda doc: unidecode(doc), # elimina acentos antes de tokenizar
    token_pattern=r'(?u)\b\w+\b'             # conserva solo tokens alfanuméricos
)

# 5) Ajustar y transformar el corpus
X_pp = vec_pp.fit_transform(texts_pp)

# 6) Sumar recuentos de cada bigrama
bigrams_pp = vec_pp.get_feature_names_out()
counts_pp = X_pp.sum(axis=0).A1

# 7) Crear DataFrame ordenado y quedarnos con top 30
df_bi_pp = (
    pd.DataFrame({'Bigrama': bigrams_pp, 'Recuento': counts_pp})
      .sort_values('Recuento', ascending=False)
      .head(30)
)

# 8) Graficar con etiquetas legibles
plt.figure(figsize=(14, 6))
plt.bar(df_bi_pp['Bigrama'], df_bi_pp['Recuento'])
plt.xticks(rotation=45, ha='right')          # rotar 45° y alinear a la derecha
plt.subplots_adjust(bottom=0.25)              # dejar más espacio abajo
plt.ylabel('Recuento')
plt.title('Top 30 bigramas PP')
plt.tight_layout()
plt.show()

# 1) Mostrar todo el top 30 con sus recuentos
print(df_bi_pp)

# 1) Filtrar solo los programas del PSOE
df_psoe = df_agrupado[df_agrupado['Partido'] == 'PSOE']

# 2) Sacar la columna lematizada y eliminar posibles NaN
texts_psoe = df_psoe['Texto lematizado'].dropna().tolist()

# 3) Convertir el set de stopwords a lista (si lo usas)
mis_stopwords_list = list(mis_stopwords)

# 4) Configurar CountVectorizer para solo bigramas sin acentos
vec_psoe = CountVectorizer(
    ngram_range=(2, 2),                      # solo bigramas
    stop_words=mis_stopwords_list,           # lista de stopwords personalizada
    preprocessor=lambda doc: unidecode(doc), # elimina acentos antes de tokenizar
    token_pattern=r'(?u)\b\w+\b'             # conserva solo tokens alfanuméricos
)

# 5) Ajustar y transformar el corpus
X_psoe = vec_psoe.fit_transform(texts_psoe)

# 6) Sumar recuentos de cada bigrama
bigrams_psoe = vec_psoe.get_feature_names_out()
counts_psoe = X_psoe.sum(axis=0).A1

# 7) Crear DataFrame ordenado y quedarnos con top 30
df_bi_psoe = (
    pd.DataFrame({'Bigrama': bigrams_psoe, 'Recuento': counts_psoe})
      .sort_values('Recuento', ascending=False)
      .head(30)
)

# 8) Graficar con etiquetas legibles
plt.figure(figsize=(14, 6))
plt.bar(df_bi_psoe['Bigrama'], df_bi_psoe['Recuento'])
plt.xticks(rotation=45, ha='right')          # rotar 45° y alinear a la derecha
plt.subplots_adjust(bottom=0.25)              # dejar más espacio abajo
plt.ylabel('Recuento bruto')
plt.title('Top 30 bigramas PSOE')
plt.tight_layout()
plt.show()

# 1) Mostrar todo el top 30 con sus recuentos
print(df_bi_psoe)

# 1) Filtrar solo los programas del PP
df_pp = df_agrupado[df_agrupado['Partido'] == 'PP']

# 2) Sacar la columna lematizada y eliminar posibles NaN
texts_pp = df_pp['Texto lematizado'].dropna().tolist()

# 3) Convertir el set de stopwords a lista (si lo usas)
mis_stopwords_list = list(mis_stopwords)

# 4) Configurar CountVectorizer para solo trigramas sin acentos
vec_pp_tri = CountVectorizer(
    ngram_range=(3, 3),                     # solo trigramas
    stop_words=mis_stopwords_list,          # lista de stopwords personalizada
    preprocessor=lambda doc: unidecode(doc),# elimina acentos antes de tokenizar
    token_pattern=r'(?u)\b\w+\b'            # conserva solo tokens alfanuméricos
)

# 5) Ajustar y transformar el corpus
X_pp_tri = vec_pp_tri.fit_transform(texts_pp)

# 6) Sumar recuentos de cada trigrama
trigrams_pp     = vec_pp_tri.get_feature_names_out()
counts_pp_tri   = X_pp_tri.sum(axis=0).A1

# 7) Crear DataFrame ordenado
df_tri_pp = pd.DataFrame({
    'Trigrama': trigrams_pp,
    'Recuento': counts_pp_tri
}).sort_values('Recuento', ascending=False)

# 7b) Eliminar cualquier trigrama que contenga 'trav'
df_tri_pp = df_tri_pp[~df_tri_pp['Trigrama'].str.contains(r'\btrav\b')]

# 8) Quedarnos con el top 30
df_tri_pp = df_tri_pp.head(30)

# 9) Graficar con etiquetas legibles
plt.figure(figsize=(14, 6))
plt.bar(df_tri_pp['Trigrama'], df_tri_pp['Recuento'])
plt.xticks(rotation=45, ha='right')         # rotar 45° y alinear a la derecha
plt.subplots_adjust(bottom=0.25)             # dejar más espacio abajo
plt.ylabel('Recuento')
plt.title('Top 30 trigramas PP')
plt.tight_layout()
plt.show()

# 1) Mostrar todo el top 30 de trigramas del PP con sus recuentos
print(df_tri_pp)

# 1) Filtrar solo los programas del PSOE
df_psoe = df_agrupado[df_agrupado['Partido'] == 'PSOE']

# 2) Sacar la columna lematizada y eliminar posibles NaN
texts_psoe = df_psoe['Texto lematizado'].dropna().tolist()

# 3) Convertir el set de stopwords a lista (si lo usas)
mis_stopwords_list = list(mis_stopwords)

# 4) Configurar CountVectorizer para solo trigramas sin acentos
vec_psoe_tri = CountVectorizer(
    ngram_range=(3, 3),                     # solo trigramas
    stop_words=mis_stopwords_list,          # lista de stopwords personalizada
    preprocessor=lambda doc: unidecode(doc),# elimina acentos antes de tokenizar
    token_pattern=r'(?u)\b\w+\b'            # conserva solo tokens alfanuméricos
)

# 5) Ajustar y transformar el corpus
X_psoe_tri = vec_psoe_tri.fit_transform(texts_psoe)

# 6) Sumar recuentos de cada trigrama
trigrams_psoe = vec_psoe_tri.get_feature_names_out()
counts_psoe_tri = X_psoe_tri.sum(axis=0).A1

# 7) Crear DataFrame ordenado y quedarnos con top 30
df_tri_psoe = (
    pd.DataFrame({'Trigrama': trigrams_psoe, 'Recuento': counts_psoe_tri})
      .sort_values('Recuento', ascending=False)
      .head(30)
)

# 8) Graficar con etiquetas legibles
plt.figure(figsize=(14, 6))
plt.bar(df_tri_psoe['Trigrama'], df_tri_psoe['Recuento'])
plt.xticks(rotation=45, ha='right')         # rotar 45° y alinear a la derecha
plt.subplots_adjust(bottom=0.25)             # dejar más espacio abajo
plt.ylabel('Recuento')
plt.title('Top 30 trigramas PSOE')
plt.tight_layout()
plt.show()

# 1) Mostrar todo el top 30 de trigramas del PSOE con sus recuentos
print(df_tri_psoe)

"""ANÁLISIS BIGRAMAS Y TRIGRAMAS: COMPARATIVA TEMPORAL"""

# 1) Filtrar todos los programas con Año < 1997
df_pre97 = df_agrupado[df_agrupado['Año'] < 1997]

# 2) Extraer la columna lematizada y eliminar posibles NaN
texts_pre97 = df_pre97['Texto lematizado'].dropna().tolist()

# 3) Vectorizador de recuentos para bigramas SIN ACENTOS
vec_pre97 = CountVectorizer(
    ngram_range=(2, 2),
    token_pattern=r'(?u)\b\w+\b',
    preprocessor=lambda doc: unidecode(doc)  # elimina acentos antes de tokenizar
)

# 4) Ajustar y transformar el corpus
X_pre97 = vec_pre97.fit_transform(texts_pre97)

# 5) Sumar recuentos de cada bigrama
bigrams_pre97 = vec_pre97.get_feature_names_out()
counts_pre97 = X_pre97.sum(axis=0).A1
freq_dict_pre97 = dict(zip(bigrams_pre97, counts_pre97))

# 6) Ordenar y quedarnos con los 30 bigramas más frecuentes
top30_pre97 = sorted(freq_dict_pre97.items(), key=lambda x: x[1], reverse=True)[:30]
df_bi_pre97 = pd.DataFrame(top30_pre97, columns=['Bigrama','Recuento'])

# 7) Graficar
plt.figure(figsize=(12,6))
plt.bar(df_bi_pre97['Bigrama'], df_bi_pre97['Recuento'])
plt.xticks(rotation=90, ha='right')
plt.subplots_adjust(bottom=0.25)
plt.ylabel('Recuento')
plt.title('Top 30 bigramas < 1997')
plt.tight_layout()
plt.show()

# 1) Mostrar todo el top 30 de bigramas de programas con Año < 1997 con sus recuentos
print(df_bi_pre97)

# 1) Filtrar todos los programas con Año < 1997
df_pre97 = df_agrupado[df_agrupado['Año'] < 1997]

# 2) Extraer la columna lematizada y eliminar posibles NaN
texts_pre97 = df_pre97['Texto lematizado'].dropna().tolist()

# 3) Vectorizador de recuentos para trigramas SIN ACENTOS
vec_tri_pre97 = CountVectorizer(
    ngram_range=(3, 3),                     # solo trigramas
    token_pattern=r'(?u)\b\w+\b',           # conserva solo tokens alfanuméricos
    preprocessor=lambda doc: unidecode(doc) # elimina acentos antes de tokenizar
)

# 4) Ajustar y transformar el corpus
X_tri_pre97 = vec_tri_pre97.fit_transform(texts_pre97)

# 5) Sumar recuentos de cada trigrama
trigrams_pre97 = vec_tri_pre97.get_feature_names_out()
counts_tri_pre97 = X_tri_pre97.sum(axis=0).A1

# 6) Crear DataFrame ordenado y quedarnos con top 30
df_tri_pre97 = (
    pd.DataFrame({'Trigrama': trigrams_pre97, 'Recuento': counts_tri_pre97})
      .sort_values('Recuento', ascending=False)
      .head(30)
)

# 7) Graficar con etiquetas legibles
plt.figure(figsize=(12, 6))
plt.bar(df_tri_pre97['Trigrama'], df_tri_pre97['Recuento'])
plt.xticks(rotation=90, ha='right')       # rotar 90° y alinear a la derecha
plt.subplots_adjust(bottom=0.3)            # dejar más espacio abajo
plt.ylabel('Recuento')
plt.title('Top 30 trigramas < 1997')
plt.tight_layout()
plt.show()

# 1) Mostrar todo el top 30 de trigramas de programas con Año < 1997 con sus recuentos
print(df_tri_pre97)

# Función para eliminar acentos con unicodedata
def strip_accents(text):
    return ''.join(
        c for c in unicodedata.normalize('NFKD', text)
        if not unicodedata.combining(c)
    )

# 1) Filtrar programas con Año entre 1998 y 2014 inclusive
df_98_14 = df_agrupado[(df_agrupado['Año'] >= 1998) & (df_agrupado['Año'] <= 2014)]

# 2) Extraer la columna lematizada y eliminar posibles NaN
texts_98_14 = df_98_14['Texto lematizado'].dropna().tolist()

# 3) Vectorizador de recuentos para bigramas SIN ACENTOS
vec_98_14 = CountVectorizer(
    ngram_range=(2, 2),                     # solo bigramas
    token_pattern=r'(?u)\b\w+\b',           # conserva solo tokens alfanuméricos
    preprocessor=lambda doc: strip_accents(doc) # elimina acentos antes de tokenizar
)

# 4) Ajustar y transformar el corpus
X_98_14 = vec_98_14.fit_transform(texts_98_14)

# 5) Sumar recuentos de cada bigrama
bigrams_98_14 = vec_98_14.get_feature_names_out()
counts_98_14 = X_98_14.sum(axis=0).A1

# 6) Crear DataFrame completo de bigramas
df_bi_98_14 = pd.DataFrame({
    'Bigrama': bigrams_98_14,
    'Recuento': counts_98_14
})

# 7) Eliminar el bigrama no deseado "eaj pnv"
df_bi_98_14 = df_bi_98_14[df_bi_98_14['Bigrama'] != 'eaj pnv']

# 8) Ordenar y quedarnos con los 30 bigramas más frecuentes
df_bi_98_14 = df_bi_98_14.sort_values('Recuento', ascending=False).head(30)

# 9) Graficar con etiquetas legibles
plt.figure(figsize=(12,6))
plt.bar(df_bi_98_14['Bigrama'], df_bi_98_14['Recuento'])
plt.xticks(rotation=45, ha='right')
plt.subplots_adjust(bottom=0.25)
plt.ylabel('Recuento')
plt.title('Top 30 bigramas entre 1998 y 2014')
plt.tight_layout()
plt.show()

# 1) Mostrar todo el top 30 de bigramas entre 1998 y 2014 con sus recuentos
print(df_bi_98_14)

# 1) Filtrar programas con Año entre 1998 y 2014 inclusive
df_98_14 = df_agrupado[(df_agrupado['Año'] >= 1998) & (df_agrupado['Año'] <= 2014)]

# 2) Extraer la columna lematizada y eliminar posibles NaN
texts_98_14 = df_98_14['Texto lematizado'].dropna().tolist()

# 3) Vectorizador de recuentos para trigramas SIN ACENTOS
vec_tri_98_14 = CountVectorizer(
    ngram_range=(3, 3),                     # solo trigramas
    token_pattern=r'(?u)\b\w+\b',           # conserva solo tokens alfanuméricos
    preprocessor=lambda doc: unidecode(doc) # elimina acentos antes de tokenizar
)

# 4) Ajustar y transformar el corpus
X_tri_98_14 = vec_tri_98_14.fit_transform(texts_98_14)

# 5) Sumar recuentos de cada trigrama
trigrams_98_14   = vec_tri_98_14.get_feature_names_out()
counts_tri_98_14 = X_tri_98_14.sum(axis=0).A1

# 6) Crear DataFrame ordenado y eliminar el trigrama no deseado
df_tri_98_14 = (
    pd.DataFrame({'Trigrama': trigrams_98_14, 'Recuento': counts_tri_98_14})
      .sort_values('Recuento', ascending=False)
)

# 6b) Filtrar fuera el trigrama exacto "eaj pnv apuesta"
df_tri_98_14 = df_tri_98_14[df_tri_98_14['Trigrama'] != 'eaj pnv apuesta']

# 7) Quedarnos con los 30 primeros tras la limpieza
df_tri_98_14 = df_tri_98_14.head(30)

# 8) Graficar con etiquetas legibles
plt.figure(figsize=(12, 6))
plt.bar(df_tri_98_14['Trigrama'], df_tri_98_14['Recuento'])
plt.xticks(rotation=45, ha='right')       # rotar 45° y alinear a la derecha
plt.subplots_adjust(bottom=0.3)            # dejar más espacio abajo
plt.ylabel('Recuento')
plt.title('Top 30 trigramas entre 1998 y 2014')
plt.tight_layout()
plt.show()

# 1) Mostrar todo el top 30 de trigramas entre 1998 y 2014 con sus recuentos
print(df_tri_98_14)

# Función para eliminar acentos con unicodedata
def strip_accents(text):
    return ''.join(
        c for c in unicodedata.normalize('NFKD', text)
        if not unicodedata.combining(c)
    )

# 1) Filtrar programas con Año >= 2015
df_2015_plus = df_agrupado[df_agrupado['Año'] >= 2015]

# 2) Extraer la columna lematizada y eliminar posibles NaN
texts_2015_plus = df_2015_plus['Texto lematizado'].dropna().tolist()

# 3) Convertir tu set de stopwords a lista (si lo usas)
mis_stopwords_list = list(mis_stopwords)

# 4) Vectorizador de recuentos para bigramas SIN ACENTOS
vec_2015_plus = CountVectorizer(
    ngram_range=(2, 2),
    stop_words=mis_stopwords_list,
    preprocessor=lambda doc: strip_accents(doc),  # elimina acentos antes de tokenizar
    token_pattern=r'(?u)\b\w+\b'
)

# 5) Ajustar y transformar el corpus
X_2015_plus = vec_2015_plus.fit_transform(texts_2015_plus)

# 6) Sumar recuentos de cada bigrama
bigrams_2015_plus = vec_2015_plus.get_feature_names_out()
counts_2015_plus = X_2015_plus.sum(axis=0).A1

# 7) Crear DataFrame y quedarnos con top 30
df_bi_2015_plus = (
    pd.DataFrame({'Bigrama': bigrams_2015_plus, 'Recuento': counts_2015_plus})
      .sort_values('Recuento', ascending=False)
      .head(30)
)

# 8) Graficar con etiquetas legibles
plt.figure(figsize=(14, 6))
plt.bar(df_bi_2015_plus['Bigrama'], df_bi_2015_plus['Recuento'])
plt.xticks(rotation=45, ha='right')
plt.subplots_adjust(bottom=0.25)
plt.ylabel('Recuento')
plt.title('Top 30 bigramas ≥ 2015')
plt.tight_layout()
plt.show()

# 1) Mostrar todo el top 30 de bigramas desde 2015 con sus recuentos
print(df_bi_2015_plus)

# Función para eliminar acentos con unicodedata
def strip_accents(text):
    return ''.join(
        c for c in unicodedata.normalize('NFKD', text)
        if not unicodedata.combining(c)
    )

# 1) Filtrar programas con Año ≥ 2015
df_2015_plus = df_agrupado[df_agrupado['Año'] >= 2015]

# 2) Extraer la columna lematizada y eliminar posibles NaN
texts_2015_plus = df_2015_plus['Texto lematizado'].dropna().tolist()

# 3) Convertir tu set de stopwords a lista (si lo usas)
mis_stopwords_list = list(mis_stopwords)

# 4) Vectorizador de recuentos para **trigramas** SIN ACENTOS
vec_tri_2015_plus = CountVectorizer(
    ngram_range=(3, 3),                     # solo trigramas
    stop_words=mis_stopwords_list,          # lista de stopwords personalizada
    preprocessor=lambda doc: strip_accents(doc),  # elimina acentos antes de tokenizar
    token_pattern=r'(?u)\b\w+\b'            # conserva solo tokens alfanuméricos
)

# 5) Ajustar y transformar el corpus
X_tri_2015_plus = vec_tri_2015_plus.fit_transform(texts_2015_plus)

# 6) Sumar recuentos de cada trigrama
trigrams_2015_plus = vec_tri_2015_plus.get_feature_names_out()
counts_tri_2015_plus = X_tri_2015_plus.sum(axis=0).A1

# 7) (Opcional) Eliminar trigramas no deseados
extras_tri = {'vehicu el electrico', 'vear apartado garantia'}
filtered_tri = [
    (k, v) for k, v in zip(trigrams_2015_plus, counts_tri_2015_plus)
    if k not in extras_tri
]

# 8) Ordenar y quedarnos con los 30 trigramas más frecuentes
top30_tri = sorted(filtered_tri, key=lambda x: x[1], reverse=True)[:30]
df_tri_2015_plus = pd.DataFrame(top30_tri, columns=['Trigrama','Recuento'])

# 9) Graficar con etiquetas legibles
plt.figure(figsize=(14, 6))
plt.bar(df_tri_2015_plus['Trigrama'], df_tri_2015_plus['Recuento'])
plt.xticks(rotation=45, ha='right')       # rotar 45° y alinear a la derecha
plt.subplots_adjust(bottom=0.3)            # dejar más espacio abajo
plt.ylabel('Recuento')
plt.title('Top 30 trigramas ≥ 2015')
plt.tight_layout()
plt.show()

# 1) Mostrar todo el top 30 de trigramas desde 2015 con sus recuentos
print(df_tri_2015_plus)

"""ANÁLISIS BIGRAMAS Y TRIGRAMAS: COMPARATIVA IDEOLÓGICA"""

# Función para eliminar acentos con unicodedata
def strip_accents(text):
    return ''.join(
        c for c in unicodedata.normalize('NFKD', text)
        if not unicodedata.combining(c)
    )

# 1) Filtrar programas con Ideología ≤ 5.0
df_low_ideol = df_agrupado[df_agrupado['Ideología'] <= 5.0]

# 2) Sacar la columna lematizada y eliminar posibles NaN
texts_low_ideol = df_low_ideol['Texto lematizado'].dropna().tolist()

# 3) Convertir tu set de stopwords a lista (si lo usas)
mis_stopwords_list = list(mis_stopwords)

# 4) Configurar CountVectorizer para solo bigramas SIN acentos
vec_low_ideol = CountVectorizer(
    ngram_range=(2, 2),                      # solo bigramas
    stop_words=mis_stopwords_list,           # lista de stopwords personalizada
    preprocessor=lambda doc: strip_accents(doc),  # elimina acentos antes de tokenizar
    token_pattern=r'(?u)\b\w+\b'             # conserva solo tokens alfanuméricos
)

# 5) Ajustar y transformar el corpus
X_low_ideol = vec_low_ideol.fit_transform(texts_low_ideol)

# 6) Sumar recuentos de cada bigrama
bigrams_low_ideol = vec_low_ideol.get_feature_names_out()
counts_low_ideol = X_low_ideol.sum(axis=0).A1

# 7) (Opcional) Eliminar bigramas no deseados
extras = {'vehicu el'}  # asegúrate de usar la forma sin acento
filtered_low = [
    (big, cnt) for big, cnt in zip(bigrams_low_ideol, counts_low_ideol)
    if big not in extras
]

# 8) Ordenar y quedarnos con los 30 bigramas más frecuentes
top30_low = sorted(filtered_low, key=lambda x: x[1], reverse=True)[:30]
df_bi_low_ideol = pd.DataFrame(top30_low, columns=['Bigrama','Recuento'])

# 9) Graficar con etiquetas legibles
plt.figure(figsize=(14, 6))
plt.bar(df_bi_low_ideol['Bigrama'], df_bi_low_ideol['Recuento'])
plt.xticks(rotation=45, ha='right')       # rotar 45° y alinear a la derecha
plt.subplots_adjust(bottom=0.25)            # dejar espacio para las etiquetas
plt.ylabel('Recuento')
plt.title('Top 30 bigramas para Ideología ≤ 5.0')
plt.tight_layout()
plt.show()

# 1) Mostrar todo el top 30 de bigramas para Ideología ≤ 5.0 con sus recuentos
print(df_bi_low_ideol)

# Función para eliminar acentos con unicodedata
def strip_accents(text):
    return ''.join(
        c for c in unicodedata.normalize('NFKD', text)
        if not unicodedata.combining(c)
    )

# 1) Filtrar programas con Ideología > 5.0
df_high_ideol = df_agrupado[df_agrupado['Ideología'] > 5.0]

# 2) Extraer la columna lematizada y eliminar posibles NaN
texts_high_ideol = df_high_ideol['Texto lematizado'].dropna().tolist()

# 3) Convertir tu set de stopwords a lista (si lo usas)
mis_stopwords_list = list(mis_stopwords)

# 4) Configurar CountVectorizer para solo bigramas SIN ACENTOS
vec_high_ideol = CountVectorizer(
    ngram_range=(2, 2),                      # solo bigramas
    stop_words=mis_stopwords_list,           # lista de stopwords personalizada
    preprocessor=lambda doc: strip_accents(doc),  # elimina acentos antes de tokenizar
    token_pattern=r'(?u)\b\w+\b'             # conserva solo tokens alfanuméricos
)

# 5) Ajustar y transformar el corpus
X_high_ideol = vec_high_ideol.fit_transform(texts_high_ideol)

# 6) Sumar recuentos de cada bigrama
bigrams_high_ideol = vec_high_ideol.get_feature_names_out()
counts_high_ideol = X_high_ideol.sum(axis=0).A1

# 7) Filtrar el bigrama no deseado "eaj pnv"
filtered_high = [
    (big, cnt)
    for big, cnt in zip(bigrams_high_ideol, counts_high_ideol)
    if big != 'eaj pnv'
]

# 8) Ordenar y quedarnos con los 30 bigramas más frecuentes
top30_high = sorted(filtered_high, key=lambda x: x[1], reverse=True)[:30]
df_bi_high_ideol = pd.DataFrame(top30_high, columns=['Bigrama', 'Recuento'])

# 9) Graficar con etiquetas legibles
plt.figure(figsize=(14, 6))
plt.bar(df_bi_high_ideol['Bigrama'], df_bi_high_ideol['Recuento'])
plt.xticks(rotation=45, ha='right')
plt.subplots_adjust(bottom=0.25)
plt.ylabel('Recuento')
plt.title('Top 30 bigramas para Ideología > 5.0')
plt.tight_layout()
plt.show()

# 1) Mostrar todo el top 30 de bigramas para Ideología > 5.0 con sus recuentos
print(df_bi_high_ideol)

# Función para eliminar acentos con unicodedata
def strip_accents(text):
    return ''.join(
        c for c in unicodedata.normalize('NFKD', text)
        if not unicodedata.combining(c)
    )

# 1) Filtrar programas con Ideología < 5.0
df_low_ideol = df_agrupado[df_agrupado['Ideología'] <= 5.0]

# 2) Extraer la columna lematizada y eliminar posibles NaN
texts_low_ideol = df_low_ideol['Texto lematizado'].dropna().tolist()

# 3) Convertir tu set de stopwords a lista (si lo utilizas)
mis_stopwords_list = list(mis_stopwords)

# 4) Configurar CountVectorizer para solo trigramas SIN ACENTOS
vec_tri_low_ideol = CountVectorizer(
    ngram_range=(3, 3),                      # solo trigramas
    stop_words=mis_stopwords_list,           # lista de stopwords personalizada
    preprocessor=lambda doc: strip_accents(doc),  # elimina acentos antes de tokenizar
    token_pattern=r'(?u)\b\w+\b'             # conserva solo tokens alfanuméricos
)

# 5) Ajustar y transformar el corpus
X_tri_low_ideol = vec_tri_low_ideol.fit_transform(texts_low_ideol)

# 6) Sumar recuentos de cada trigrama
trigrams_low_ideol = vec_tri_low_ideol.get_feature_names_out()
counts_tri_low_ideol = X_tri_low_ideol.sum(axis=0).A1

# 7) Crear DataFrame ordenado y quedarnos con los 30 trigramas más frecuentes
df_tri_low_ideol = (
    pd.DataFrame({'Trigrama': trigrams_low_ideol, 'Recuento': counts_tri_low_ideol})
      .sort_values('Recuento', ascending=False)
      .head(30)
)

# 8) Graficar con etiquetas legibles
plt.figure(figsize=(14, 6))
plt.bar(df_tri_low_ideol['Trigrama'], df_tri_low_ideol['Recuento'])
plt.xticks(rotation=45, ha='right')       # rotar 45° y alinear a la derecha
plt.subplots_adjust(bottom=0.3)            # dejar espacio para las etiquetas
plt.ylabel('Recuento')
plt.title('Top 30 trigramas para Ideología < 5.0')
plt.tight_layout()
plt.show()

# 1) Mostrar todo el top 30 de trigramas para Ideología < 5.0 con sus recuentos
print(df_tri_low_ideol)

# Función para eliminar acentos con unicodedata
def strip_accents(text):
    return ''.join(
        c for c in unicodedata.normalize('NFKD', text)
        if not unicodedata.combining(c)
    )

# 1) Filtrar programas con Ideología > 5.0
df_high_ideol = df_agrupado[df_agrupado['Ideología'] > 5.0]

# 2) Extraer la columna lematizada y eliminar posibles NaN
texts_high_ideol = df_high_ideol['Texto lematizado'].dropna().tolist()

# 3) Preparar lista de stopwords (si la usas)
mis_stopwords_list = list(mis_stopwords)

# 4) Configurar CountVectorizer para trigramas SIN ACENTOS
vec_tri_high_ideol = CountVectorizer(
    ngram_range=(3, 3),
    stop_words=mis_stopwords_list,
    preprocessor=lambda doc: strip_accents(doc),
    token_pattern=r'(?u)\b\w+\b'
)

# 5) Ajustar y transformar el corpus
X_tri_high_ideol = vec_tri_high_ideol.fit_transform(texts_high_ideol)

# 6) Sumar recuentos de cada trigrama
trigrams_high_ideol = vec_tri_high_ideol.get_feature_names_out()
counts_tri_high_ideol = X_tri_high_ideol.sum(axis=0).A1

# 7) Crear DataFrame ordenado
df_tri_high_ideol = pd.DataFrame({
    'Trigrama': trigrams_high_ideol,
    'Recuento': counts_tri_high_ideol
}).sort_values('Recuento', ascending=False)

# **7b) Eliminar el trigrama exacto "eaj pnv apuesta"**
df_tri_high_ideol = df_tri_high_ideol[
    df_tri_high_ideol['Trigrama'] != 'eaj pnv apuesta'
]

# 8) Quedarnos con los 30 primeros tras filtrar
df_tri_high_ideol = df_tri_high_ideol.head(30)

# 9) Graficar con etiquetas legibles
plt.figure(figsize=(14, 6))
plt.bar(df_tri_high_ideol['Trigrama'], df_tri_high_ideol['Recuento'])
plt.xticks(rotation=45, ha='right')
plt.subplots_adjust(bottom=0.3)
plt.ylabel('Recuento')
plt.title('Top 30 trigramas para Ideología > 5.0')
plt.tight_layout()
plt.show()

# 1) Mostrar todo el top 30 de trigramas para Ideología > 5.0 con sus recuentos
print(df_tri_high_ideol)

"""EVOLUCIÓN TEMPORAL DE LA LONGITUD PROGRAMAS ELECTORALES"""

# 1) Agrupar y sumar
df_total = (
    df_agrupado
      .groupby('Año', sort=False)['Longitud palabras']
      .sum()
      .reset_index(name='Total palabras')
)

# 2) Ordenar por floating año para conservar cronología
df_total['Año_f'] = df_total['Año'].astype(float)
df_total.sort_values('Año_f', inplace=True)

# 3) Crear posiciones equidistantes y etiquetas
labels = df_total['Año'].tolist()          # ['1977.0','1979.0',…]
x = list(range(len(labels)))                # [0,1,2,…,len-1]
y = df_total['Total palabras'].values

# 4) Plot usando posiciones
plt.figure(figsize=(12,6))
plt.plot(x, y, marker='o', linestyle='-', linewidth=2, markersize=6)

# 5) Asignar ticks equidistantes con las labels
plt.xticks(x, labels, rotation=45, ha='right')

plt.xlabel('Año', fontsize=12)
plt.ylabel('Total de Caracteres', fontsize=12)
plt.title('Evolución temporal', fontsize=14)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# Asegurarse de que la columna 'Año' esté correctamente como texto
df_agrupado['Año'] = df_agrupado['Año'].astype(str)

# Paso 1: Calcular el número medio de caracteres por año
df_media_caracteres = df_agrupado.groupby('Año')['Longitud palabras'].mean()

# Paso 2: Graficar el histograma
plt.figure(figsize=(10, 6))  # Tamaño de la figura
plt.plot(df_media_caracteres.index, df_media_caracteres.values, marker='o', linestyle='-', color='b')

# Añadir etiquetas y título
plt.xlabel('Año')
plt.ylabel('Número medio de caracteres')
plt.title('Número medio de caracteres de los programas políticos por año')
plt.grid(True)

# Asegurarse de que los años estén ordenados cronológicamente
plt.xticks(df_media_caracteres.index, rotation=45)  # Rotamos las etiquetas para mayor claridad

# Mostrar el gráfico
plt.show()

"""EVOLUCIÓN TEMPORAL DE LA LONGITUD DE LA CUESTIÓN MEDIOAMBIENTAL EN LOS PROGRAMAS ELECTORALES"""

# --- 0) Crear la serie serie_car_pag si no existe aún ---
# Agrupamos por sub-año y calculamos la media de caracteres por página
serie_car_pag = (
    df_agrupado
      .groupby('Año', sort=False)['Nº caracteres/página']
      .mean()
)

# --- 1) Convertir la Serie en DataFrame y resetear índice ---
df_plot = serie_car_pag.reset_index(name='Media_car_pag')

# --- 2) Crear columna float para ordenar correctamente ---
df_plot['Año_f'] = df_plot['Año'].astype(float)

# --- 3) Ordenar cronológicamente por año numérico ---
df_plot.sort_values('Año_f', inplace=True)

# --- 4) Preparar X, Y y etiquetas para espaciar uniformemente ---
x = list(range(len(df_plot)))
y = df_plot['Media_car_pag'].values
labels = df_plot['Año'].tolist()  # sub-años como strings

# --- 5) Dibujar la serie con posiciones equidistantes ---
plt.figure(figsize=(12, 6))
plt.plot(x, y, marker='o', linestyle='-', linewidth=2, markersize=6)

# --- 6) Ajustar ticks con etiquetas originales ---
plt.xticks(x, labels, rotation=45, ha='right')

plt.xlabel('Año')
plt.ylabel('Número medio de caracteres por página')
plt.title('Evolución del número medio de caracteres por página')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

"""EVOLUCIÓN TEMPORAL DE LA LONGITUD DE LA CUESTIÓN MEDIOAMBIENTAL EN LOS PROGRAMAS ELECTORALES: COMPARATIVA PARTIDOS POLÍTICOS"""

# Filtrar los datos para el PSOE
df_psoe = df_agrupado[df_agrupado['Partido'] == 'PSOE']

# Calcular el número medio de caracteres del programa por año
df_psoe_media_caracteres = df_psoe.groupby('Año')['Longitud palabras'].mean()

# Graficar la evolución del número de caracteres del programa del PSOE
plt.figure(figsize=(10, 6))  # Tamaño de la figura
plt.plot(df_psoe_media_caracteres.index, df_psoe_media_caracteres.values, marker='o', linestyle='-', color='b')

# Añadir etiquetas y título
plt.xlabel('Año')
plt.ylabel('Número de caracteres')
plt.title('Evolución del número de caracteres del programa electoral del PSOE')
plt.grid(True)

# Asegurarse de que los años estén ordenados cronológicamente
plt.xticks(df_psoe_media_caracteres.index, rotation=45)  # Rotamos las etiquetas para mayor claridad

# Mostrar el gráfico
plt.show()

# 1) Crear etiqueta de año como string (para distinguir 2019.1 vs 2019.2)
df_agrupado['Año_label'] = df_agrupado['Año'].astype(str)

# 2) Filtrar solo PSOE
df_psoe = df_agrupado[df_agrupado['Partido'] == 'PSOE'].copy()

# 3) Calcular média de caracteres por página por Año_label
serie_psoe = df_psoe.groupby('Año_label')['Nº caracteres/página'].mean()

# 4) Ordenar cronológicamente (índice numérico) y volver a string
serie_psoe.index = serie_psoe.index.astype(float)
serie_psoe = serie_psoe.sort_index()
serie_psoe.index = serie_psoe.index.astype(str)

# 5) Crear posiciones equidistantes para el eje X
x = list(range(len(serie_psoe)))

# 6) Graficar
plt.figure(figsize=(10,6))
plt.plot(x, serie_psoe.values, marker='o', linestyle='-', color='b')
plt.xticks(x, serie_psoe.index, rotation=45)
plt.xlabel('Año')
plt.ylabel('Número de caracteres por página')
plt.title('Evolución del número de caracteres/página del PSOE')
plt.grid(True)
plt.tight_layout()
plt.show()

# Asegurarse de que 'Año' esté como texto
df_agrupado['Año'] = df_agrupado['Año'].astype(str)

# Filtrar los datos para el PP y Alianza Popular entre 1977 y 1986
df_pp = df_agrupado[(df_agrupado['Partido'] == 'PP') |
                    ((df_agrupado['Partido'] == 'Alianza Popular') & (df_agrupado['Año'].str.contains('^19(7[7-9]|8[0-6])')))]

# Calcular el número medio de caracteres del programa por año
df_pp_media_caracteres = df_pp.groupby('Año')['Longitud palabras'].mean()

# Graficar la evolución del número de caracteres del programa del PP (y Alianza Popular)
plt.figure(figsize=(10, 6))  # Tamaño de la figura
plt.plot(df_pp_media_caracteres.index, df_pp_media_caracteres.values, marker='o', linestyle='-', color='r')

# Añadir etiquetas y título
plt.xlabel('Año')
plt.ylabel('Número de caracteres')
plt.title('Evolución del número de caracteres del programa electoral del PP (y Alianza Popular entre 1977-1986)')
plt.grid(True)

# Asegurarse de que los años estén ordenados cronológicamente
plt.xticks(df_pp_media_caracteres.index, rotation=45)  # Rotamos las etiquetas para mayor claridad

# Mostrar el gráfico
plt.show()

# 1) Crear etiqueta de año como string para distinguir 1977–1986
df_agrupado['Año_label'] = df_agrupado['Año'].astype(str)

# 2) Filtrar solo PP y Alianza Popular entre 1977 y 1986
df_pp = df_agrupado[
    (df_agrupado['Partido']=='PP') |
    ((df_agrupado['Partido']=='Alianza Popular') & df_agrupado['Año_label'].str.contains(r'^19(7[7-9]|8[0-6])'))
]

# 3) Media de caracteres por página por año_label
serie_pp = df_pp.groupby('Año_label')['Nº caracteres/página'].mean()

# 4) Ordenar cronológicamente conservando decimales
serie_pp.index = serie_pp.index.astype(float)
serie_pp = serie_pp.sort_index()
serie_pp.index = serie_pp.index.astype(str)

# 5) Posiciones equidistantes para el eje X
x = list(range(len(serie_pp)))

# 6) Graficar
plt.figure(figsize=(10,6))
plt.plot(x, serie_pp.values, marker='o', linestyle='-', color='r')
plt.xticks(x, serie_pp.index, rotation=45)
plt.xlabel('Año')
plt.ylabel('Número de caracteres por página')
plt.title('Evolución del número de caracteres/página – PP & Alianza Popular 1977-86')
plt.grid(True)
plt.tight_layout()
plt.show()

# Filtrar los datos para VOX
df_vox = df_agrupado[df_agrupado['Partido'] == 'VOX']

# Calcular el número medio de caracteres del programa por año
df_vox_media_caracteres = df_vox.groupby('Año')['Longitud palabras'].mean()

# Graficar la evolución del número de caracteres del programa de VOX
plt.figure(figsize=(10, 6))  # Tamaño de la figura
plt.plot(df_vox_media_caracteres.index, df_vox_media_caracteres.values, marker='o', linestyle='-', color='b')

# Añadir etiquetas y título
plt.xlabel('Año')
plt.ylabel('Número de caracteres')
plt.title('Evolución del número de caracteres del programa electoral de VOX')
plt.grid(True)

# Asegurarse de que los años estén ordenados cronológicamente
plt.xticks(df_vox_media_caracteres.index, rotation=45)  # Rotamos las etiquetas para mayor claridad

# Mostrar el gráfico
plt.show()

# 1) Crear etiqueta de año como string (para distinguir 2019.1 vs 2019.2)
df_agrupado['Año_label'] = df_agrupado['Año'].astype(str)

# 2) Filtrar solo VOX
df_vox = df_agrupado[df_agrupado['Partido'] == 'VOX'].copy()

# 3) Calcular media de caracteres por página por Año_label
serie_vox = df_vox.groupby('Año_label')['Nº caracteres/página'].mean()

# 4) Ordenar cronológicamente (índice numérico) y volver a string
serie_vox.index = serie_vox.index.astype(float)
serie_vox = serie_vox.sort_index()
serie_vox.index = serie_vox.index.astype(str)

# 5) Crear posiciones equidistantes para el eje X
x = list(range(len(serie_vox)))

# 6) Graficar
plt.figure(figsize=(10,6))
plt.plot(x, serie_vox.values, marker='o', linestyle='-', color='g')
plt.xticks(x, serie_vox.index, rotation=45)
plt.xlabel('Año')
plt.ylabel('Número de caracteres por página')
plt.title('Evolución del nímero de caracteres/página de VOX')
plt.grid(True)
plt.tight_layout()
plt.show()

# Filtrar los datos para Podemos
df_podemos = df_agrupado[df_agrupado['Partido'] == 'Podemos']

# Calcular el número medio de palabras del programa por año
serie = df_podemos.groupby('Año')['Longitud palabras'].mean()

plt.figure(figsize=(10, 6))
plt.plot(serie.index, serie.values, marker='o', linestyle='-')

plt.xlabel('Año')
plt.ylabel('Número de caracteres')
plt.title('Evolución del número de caracteres del programa de Podemos')
plt.grid(True)

# Forzar eje Y desde 0 hasta un poco por encima del máximo
plt.ylim(0, serie.max() * 1.05)

# Asegurar que los ticks de X son los años de Podemos
plt.xticks(serie.index, rotation=45)

plt.tight_layout()
plt.show()

# 1) Filtrar desde 2015 y conservar etiqueta con decimal
df_reciente = df_agrupado[df_agrupado['Año'].astype(float) >= 2015.0].copy()
df_reciente['Año_label'] = df_reciente['Año'].astype(str)

# 2) Pivotar para tener cada partido en columna, índice = Año_label
pivot_lbl = (
    df_reciente
      .pivot_table(
         index='Año_label',
         columns='Partido',
         values='Longitud palabras',
         aggfunc='mean'
      )
)

# 3) Ordenar el índice numéricamente, luego volver a string
pivot_lbl.index = pivot_lbl.index.astype(float)
pivot_lbl = pivot_lbl.sort_index()
pivot_lbl.index = pivot_lbl.index.astype(str)

# 4) Quedarnos solo con las columnas de interés
pivot_ps = pivot_lbl[['Podemos','SUMAR']]

# 5) Crear posiciones equidistantes para el eje X
x = list(range(len(pivot_ps)))

# 6) Dibujar ambas series
plt.figure(figsize=(8,5))
for party in ['Podemos','SUMAR']:
    plt.plot(x, pivot_ps[party].values, marker='o', label=party)

# 7) Ajustes finales
plt.xticks(x, pivot_ps.index, rotation=45)
plt.xlabel('Año')
plt.ylabel('Número de caracteres')
plt.title('Evolución número de caracteres: Podemos vs SUMAR (2015→)')
plt.legend()
plt.grid(True)
plt.ylim(0, pivot_ps.stack().max() * 1.05)
plt.tight_layout()
plt.show()

# 1) Filtrar desde 2015 y conservar etiqueta con decimal
df_reciente = df_agrupado[df_agrupado['Año'].astype(float) >= 2015.0].copy()
df_reciente['Año_label'] = df_reciente['Año'].astype(str)

# 2) Pivotar para tener cada partido en columna, índice = Año_label, pero usando caracteres/página
pivot_chars_ppp = (
    df_reciente
      .pivot_table(
         index='Año_label',
         columns='Partido',
         values='Nº caracteres/página',
         aggfunc='mean'
      )
)

# 3) Ordenar el índice numéricamente, luego volver a string
pivot_chars_ppp.index = pivot_chars_ppp.index.astype(float)
pivot_chars_ppp = pivot_chars_ppp.sort_index()
pivot_chars_ppp.index = pivot_chars_ppp.index.astype(str)

# 4) Quedarnos solo con las columnas de interés: Podemos y SUMAR
pivot_chars_ps = pivot_chars_ppp[['Podemos','SUMAR']]

# 5) Crear posiciones equidistantes para el eje X
x = list(range(len(pivot_chars_ps)))

# 6) Dibujar ambas series
plt.figure(figsize=(8,5))
for party in ['Podemos','SUMAR']:
    plt.plot(x, pivot_chars_ps[party].values, marker='o', label=party)

# 7) Ajustes finales
plt.xticks(x, pivot_chars_ps.index, rotation=45)
plt.xlabel('Año')
plt.ylabel('Número  de caracteres por página')
plt.title('Evolución número de caracteres/página: Podemos vs SUMAR (2015→)')
plt.legend()
plt.grid(True)
plt.ylim(0, pivot_chars_ps.stack().max() * 1.05)
plt.tight_layout()
plt.show()

# Filtrar los datos para el PSOE
df_psoe = df_agrupado[df_agrupado['Partido'] == 'PSOE']

# Calcular el número medio de caracteres del programa del PSOE por año
df_psoe_media_caracteres = df_psoe.groupby('Año')['Longitud palabras'].mean()

# Filtrar los datos para el PP (y Alianza Popular entre 1977 y 1986)
df_pp = df_agrupado[(df_agrupado['Partido'] == 'PP') |
                    ((df_agrupado['Partido'] == 'Alianza Popular') & (df_agrupado['Año'].str.contains('^19(7[7-9]|8[0-6])')))]

# Calcular el número medio de caracteres del programa del PP (y Alianza Popular)
df_pp_media_caracteres = df_pp.groupby('Año')['Longitud palabras'].mean()

# Graficar la evolución de ambos partidos en el mismo gráfico
plt.figure(figsize=(10, 6))  # Tamaño de la figura

# Graficar PSOE
plt.plot(df_psoe_media_caracteres.index, df_psoe_media_caracteres.values, marker='o', linestyle='-', color='b', label='PSOE')

# Graficar PP/Alianza Popular
plt.plot(df_pp_media_caracteres.index, df_pp_media_caracteres.values, marker='o', linestyle='-', color='r', label='PP/Alianza Popular')

# Añadir etiquetas y título
plt.xlabel('Año')
plt.ylabel('Número de caracteres')
plt.title('Evolución del número de caracteres de los programas del PSOE y PP/Alianza Popular')
plt.grid(True)

# Asegurarse de que los años estén ordenados cronológicamente
plt.xticks(rotation=45)  # Rotamos las etiquetas para mayor claridad

# Añadir leyenda para distinguir las dos líneas
plt.legend()

# Mostrar el gráfico
plt.show()

# 1) Asegurarnos de tener la etiqueta string de año
df_agrupado['Año_label'] = df_agrupado['Año'].astype(str)

# 2) Serie PSOE: media de caracteres/página por Año_label
serie_psoe = (
    df_agrupado[df_agrupado['Partido']=='PSOE']
      .groupby('Año_label')['Nº caracteres/página']
      .mean()
)

# 3) Serie PP & AP 1977–86: media de caracteres/página por Año_label
mask_pp = (
    (df_agrupado['Partido']=='PP') |
    ((df_agrupado['Partido']=='Alianza Popular')
      & df_agrupado['Año_label'].str.contains(r'^19(7[7-9]|8[0-6])'))
)
serie_pp = (
    df_agrupado[mask_pp]
      .groupby('Año_label')['Nº caracteres/página']
      .mean()
)

# 4) Unir índices de ambas series y ordenar cronológicamente
all_labels = pd.Index(serie_psoe.index).union(serie_pp.index)
# convertir a float para ordenar por valor numérico, luego a string
sorted_labels = all_labels.astype(float).sort_values().astype(str)

# 5) Reindexar series para que compartan mismo índice
serie_psoe = serie_psoe.reindex(sorted_labels)
serie_pp   = serie_pp.reindex(sorted_labels)

# 6) Posiciones equidistantes en el eje X
x = range(len(sorted_labels))

# 7) Graficar ambas series en un mismo plot
plt.figure(figsize=(10,6))
plt.plot(x, serie_psoe.values, marker='o', linestyle='-', color='b', label='PSOE')
plt.plot(x, serie_pp.values,   marker='o', linestyle='-', color='r', label='PP & AP 77–86')

# 8) Ajustes de ticks y etiquetas
plt.xticks(x, sorted_labels, rotation=45)
plt.xlabel('Año')
plt.ylabel('Número de caracteres por página')
plt.title('Evolución relativa del número de caracteres de los programas del PSOE y PP/Alianza Popular')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

df_agrupado['Año'] = df_agrupado['Año'].astype(float)

# 1) Escogemos solo los partidos que nos interesan y desde 2015
parties = ['PSOE','PP','VOX','Podemos','SUMAR']
df_sel = df_agrupado[
    (df_agrupado['Partido'].isin(parties)) &
    (df_agrupado['Año'] >= 2015.0)        # ahora 'Año' es float de nuevo
].copy()

# 2) Creamos una etiqueta de año que distinga 2019.1 y 2019.2
df_sel['Año_label'] = df_sel['Año'].astype(str)

# 3) Pivotamos para tener en columnas cada partido y en índice cada año_label
pivot = df_sel.pivot_table(
    index='Año_label',
    columns='Partido',
    values='Longitud palabras',
    aggfunc='mean'
)

# 4) Ordenamos el índice numéricamente y lo volvemos a string
pivot.index = pivot.index.astype(float)
pivot = pivot.sort_index()
pivot.index = pivot.index.astype(str)

# 5) Preparamos posiciones equidistantes para el eje X
x = list(range(len(pivot)))

# 6) Dibujamos cada partido en la misma figura
plt.figure(figsize=(10,6))
for party in parties:
    if party in pivot:
        plt.plot(x, pivot[party].values, marker='o', label=party)

# 7) Ajustes finales: ticks, leyenda, límites
plt.xticks(x, pivot.index, rotation=45)
plt.xlabel('Año')
plt.ylabel('Número de caracteres')
plt.title('Evolución número de caracteres PSOE - PP - VOX - PODEMOS - SUMAR')
plt.legend(title='Partido')
plt.grid(True)
plt.ylim(0, pivot.max().max() * 1.05)
plt.tight_layout()
plt.show()

# 1) Asegurarnos de tener etiqueta string de año
df_agrupado['Año_label'] = df_agrupado['Año'].astype(str)

# 2) Filtrar los partidos de interés y desde 1977 si quieres incluir AP
parties = ['PSOE','PP','Alianza Popular','VOX','Podemos','SUMAR']
df_sel = df_agrupado[df_agrupado['Partido'].isin(parties)].copy()

# 3) Calcular media de caracteres/página por Año_label y Partido
pivot = (
    df_sel
      .pivot_table(
         index='Año_label',
         columns='Partido',
         values='Nº caracteres/página',
         aggfunc='mean'
      )
)

# 4) Ordenar cronológicamente según valor numérico del año_label
pivot.index = pivot.index.astype(float)
pivot = pivot.sort_index()
pivot.index = pivot.index.astype(str)

# 5) Unir “PP” y “Alianza Popular” en una sola serie si lo deseas
pivot['PP / AP 77–86'] = pivot.get('PP', pd.Series())\
                           .fillna(0) + pivot.get('Alianza Popular', pd.Series()).fillna(0)
pivot = pivot.drop(columns=['Alianza Popular'], errors='ignore')

# 6) Seleccionar las series finales a dibujar
to_plot = ['PSOE','PP / AP 77–86','VOX','Podemos','SUMAR']

# 7) Crear posiciones equidistantes para el eje X
x = list(range(len(pivot)))

# 8) Graficar todas las series
plt.figure(figsize=(10,6))
for party in to_plot:
    if party in pivot:
        plt.plot(x, pivot[party].values, marker='o', label=party)

# 9) Ajustes de ejes y etiquetas
plt.xticks(x, pivot.index, rotation=45)
plt.xlabel('Año')
plt.ylabel('Número de caracteres por página')
plt.title('Evolución relativa del número de caracteres/página PSOE - PP - VOX - PODEMOS - SUMAR')
plt.legend(title='Partido')
plt.grid(True)
plt.ylim(0, pivot[to_plot].stack().max() * 1.05)
plt.tight_layout()
plt.show()

"""ESTUDIO CORRELACIÓN ENTRE LA IDEOLOGÍA DE LOS PARTIDOS POLÍTICOS Y LA LONGITUD DE LA CUESTIÓN MEDIOAMBIENTAL EN SUS PROGRAMAS ELECTORALES"""

# Asegurarse de que los datos están correctamente cargados
df_agrupado['Ideología'] = df_agrupado['Ideología'].astype(float)  # Asegurarse de que Ideología sea numérica

# Ordenar por ideología de menor a mayor
df_agrupado_sorted = df_agrupado.sort_values(by='Ideología')

# Crear el gráfico de dispersión
plt.figure(figsize=(10, 6))  # Tamaño de la figura
plt.scatter(df_agrupado_sorted['Ideología'], df_agrupado_sorted['Longitud palabras'], color='b', alpha=0.6)

# Añadir etiquetas y título
plt.xlabel('Ideología')
plt.ylabel('Longitud de palabras')
plt.title('Estudio Correlación: Ideología y Longitud de los Palabras')

# Mostrar el gráfico
plt.grid(True)
plt.show()

from scipy.stats import pearsonr


ideol = df_agrupado['Ideología'].astype(float)
long = df_agrupado['Longitud palabras'].astype(float)

# 2) Cálculo con scipy
r, p_value = pearsonr(ideol, long)
r2 = r**2

print(f"Coeficiente de Pearson (r): {r:.3f}")

# Asegurarnos de que las columnas tienen el tipo adecuado
df_agrupado['Ideología'] = df_agrupado['Ideología'].astype(float)
df_agrupado['Nº caracteres/página'] = df_agrupado['Nº caracteres/página'].astype(float)

# Ordenar por ideología de menor a mayor (opcional para el scatter)
df_sorted = df_agrupado.sort_values(by='Ideología')

# Crear el scatter plot usando Nº caracteres/página en el eje Y
plt.figure(figsize=(10, 6))
plt.scatter(
    df_sorted['Ideología'],
    df_sorted['Nº caracteres/página'],
    color='b',
    alpha=0.6
)

plt.xlabel('Ideología')
plt.ylabel('Número de caracteres por página')
plt.title('Estudio Relativo Correlación: Ideología y Longitud de los Palabras')
plt.grid(True)
plt.tight_layout()
plt.show()

ideol = df_agrupado['Ideología'].astype(float)
long = df_agrupado['Nº caracteres/página'].astype(float)

# 2) Cálculo con scipy
r, p_value = pearsonr(ideol, long)
r2 = r**2

print(f"Coeficiente de Pearson (r): {r:.3f}")

"""ESTUDIO CORRELACIÓN ENTRE EL % DE ESCAÑOS OBTENIDOS POR LOS PARTIDOS POLÍTICOS Y LA LONGITUD DE LA CUESTIÓN MEDIOAMBIENTAL EN SUS PROGRAMAS ELECTORALES"""

plt.figure(figsize=(10, 6))
plt.scatter(df_agrupado['% Escaños obtenidos'], df_agrupado['Longitud palabras'], color='g', alpha=0.6)
plt.xlabel('% Escaños obtenidos')
plt.ylabel('Longitud de palabras')
plt.title('Estudio Correlación: % Escaños Obtenidos y Longitud de los Palabras')
plt.grid(True)
plt.show()

ideol = df_agrupado['% Escaños obtenidos'].astype(float)
long = df_agrupado['Longitud palabras'].astype(float)

# 2) Cálculo con scipy
r, p_value = pearsonr(ideol, long)
r2 = r**2

print(f"Coeficiente de Pearson (r): {r:.3f}")

df_agrupado['% Escaños obtenidos'] = df_agrupado['% Escaños obtenidos'].astype(float)
df_agrupado['Nº caracteres/página']    = df_agrupado['Nº caracteres/página'].astype(float)

# Dibujar scatter: % escaños vs nº medio de caracteres por página
plt.figure(figsize=(10, 6))
plt.scatter(
    df_agrupado['% Escaños obtenidos'],
    df_agrupado['Nº caracteres/página'],
    color='g',
    alpha=0.6
)
plt.xlabel('% Escaños obtenidos')
plt.ylabel('Número medio de caracteres por página')
plt.title('Estudio Relativo Correlación: % Escaños Obtenidos y Longitud de los Palabras')
plt.grid(True)
plt.tight_layout()
plt.show()

# 1) Asegúrate de tener las dos columnas numéricas
ideol = df_agrupado['% Escaños obtenidos'].astype(float)
long = df_agrupado['Nº caracteres/página'].astype(float)

# 2) Cálculo con scipy
r, p_value = pearsonr(ideol, long)
r2 = r**2

print(f"Coeficiente de Pearson (r): {r:.3f}")

"""DISTRIBUCIÓN DE LA LONGITUD DE LA CUESTIÓN MEDIOAMBIENTAL EN LOS PROGRAMAS ELECTORALES DE LOS PARTIDOS POLÍTICOS"""

import seaborn as sns

# Crear el gráfico Box Plot
plt.figure(figsize=(12, 8))  # Ajustamos el tamaño de la figura para que haya más espacio
sns.boxplot(x='Partido', y='Longitud palabras', data=df_agrupado)

# Rotar las etiquetas del eje X para evitar que se sobrepongan
plt.xticks(rotation=45, ha='right')  # Rotamos las etiquetas en el eje X y las alineamos a la derecha

# Título y etiquetas de los ejes
plt.title('Distribución del número de caracteres de los programas por partido')
plt.xlabel('Partido')
plt.ylabel('Número de caracteres')

# Mostrar la cuadrícula
plt.grid(True)

# Mostrar el gráfico
plt.tight_layout()  # Esto asegura que no haya solapamiento
plt.show()

# Calcular el IQR para detectar outliers
Q1 = df_agrupado['Longitud palabras'].quantile(0.25)
Q3 = df_agrupado['Longitud palabras'].quantile(0.75)
IQR = Q3 - Q1

# Filtrar los valores que están fuera del rango permitido
df_sin_outliers = df_agrupado[(df_agrupado['Longitud palabras'] >= (Q1 - 1.5 * IQR)) &
                              (df_agrupado['Longitud palabras'] <= (Q3 + 1.5 * IQR))]

# Crear el gráfico Box Plot sin outliers
plt.figure(figsize=(12, 8))
sns.boxplot(x='Partido', y='Longitud palabras', data=df_sin_outliers)

# Rotar las etiquetas del eje X para evitar que se sobrepongan
plt.xticks(rotation=45, ha='right')

# Título y etiquetas de los ejes
plt.title('Distribución del número de caracteres de los programas por partido (sin outliers)')
plt.xlabel('Partido')
plt.ylabel('Número de caracteres')

# Mostrar la cuadrícula
plt.grid(True)

# Mostrar el gráfico
plt.tight_layout()
plt.show()

# 1) Calcular la media de ideología por partido (sobre el DataFrame sin outliers)
ideol_media = (
    df_sin_outliers
      .groupby('Partido')['Ideología']
      .mean()
      .sort_values()        # de menor a mayor
)

# 2) Lista de partidos ordenada
order_partidos = ideol_media.index.tolist()

# 3) Crear el boxplot indicando ese orden
plt.figure(figsize=(12, 8))
sns.boxplot(
    x='Partido',
    y='Longitud palabras',
    data=df_sin_outliers,
    order=order_partidos    # <— aquí le pasas tu orden personalizado
)

# Rotar etiquetas y ajustar
plt.xticks(rotation=45, ha='right')
plt.title('Distribución del número de caracteres de los programas por partido (sin outliers)\nordenado por puntuación ideológica')
plt.xlabel('Partido (ordenado por ideología media)')
plt.ylabel('Número de caracteres')
plt.grid(True, axis='y')
plt.tight_layout()
plt.show()

# 1) Detectar outliers sobre Nº caracteres/página usando IQR
Q1 = df_agrupado['Nº caracteres/página'].quantile(0.25)
Q3 = df_agrupado['Nº caracteres/página'].quantile(0.75)
IQR = Q3 - Q1

df_sin_outliers_cp = df_agrupado[
    (df_agrupado['Nº caracteres/página'] >= (Q1 - 1.5 * IQR)) &
    (df_agrupado['Nº caracteres/página'] <= (Q3 + 1.5 * IQR))
]

# 2) Media de ideología por partido (sin outliers)
ideol_media = (
    df_sin_outliers_cp
      .groupby('Partido')['Ideología']
      .mean()
      .sort_values()
)
order_partidos = ideol_media.index.tolist()

# 3) Boxplot de Nº caracteres/página ordenado por ideología media
plt.figure(figsize=(12, 8))
sns.boxplot(
    x='Partido',
    y='Nº caracteres/página',
    data=df_sin_outliers_cp,
    order=order_partidos
)

plt.xticks(rotation=45, ha='right')
plt.title('Distribución Relativa del número de caracteres por página de los programas por partido (sin outliers)\nordenado por puntuación ideológica')
plt.xlabel('Partido (ordenado por ideología media)')
plt.ylabel('Número de caracteres por página')
plt.grid(True, axis='y')
plt.tight_layout()
plt.show()

"""LONGITUD MEDIA DE LA CUESTIÓN MEDIOAMBIENTAL EN LOS PROGRAMAS ELECTORALES DE LOS PARTIDOS POLÍTICOS"""

#Hacemos un gráfico de barras para ver la longitud media de los programas#
df_media_caracteres_partido = df_agrupado.groupby('Partido')['Longitud palabras'].mean()
plt.figure(figsize=(10, 6))
df_media_caracteres_partido.plot(kind='bar', color='skyblue')
plt.title('Longitud media de los programas electorales')
plt.xlabel('Partido')
plt.ylabel('Número de caracteres')
plt.grid(True)
plt.show()

# 1) Asegurarnos de que la columna “Nº caracteres/página” es numérica
df_agrupado['Nº caracteres/página'] = df_agrupado['Nº caracteres/página'].astype(float)

# 2) Calcular el número medio de caracteres por página para cada partido
df_media_chars_pag = df_agrupado.groupby('Partido')['Nº caracteres/página'].mean()

# 3) Dibujar el bar‐plot
plt.figure(figsize=(10,6))
df_media_chars_pag.plot(kind='bar', color='skyblue')
plt.title('Longitud Media Relativa de los programas electorales')
plt.xlabel('Partido')
plt.ylabel('Número de caracteres por página')
plt.grid(True, axis='y')
plt.tight_layout()
plt.show()

# Media de ideología por partido
ideol = df_agrupado.groupby('Partido')['Ideología'].mean().sort_values()

# Media de longitud reindexada según orden de ideología
long_media = df_agrupado.groupby('Partido')['Longitud palabras'].mean()
long_media = long_media.reindex(ideol.index)

# Bar-plot
plt.figure(figsize=(10,6))
long_media.plot(kind='bar', color='skyblue')
plt.title('Longitud media de los programas electorales ordenados por puntuación ideológica')
plt.xlabel('Partido')
plt.ylabel('Número de caracteres')
plt.grid(True, axis='y')
plt.tight_layout()
plt.show()

# 1) Media de ideología por partido
ideol = df_agrupado.groupby('Partido')['Ideología'].mean().sort_values()

# 2) Media de nº caracteres/página reindexada según ese orden
chars_pag_media = df_agrupado.groupby('Partido')['Nº caracteres/página'].mean()
chars_pag_media = chars_pag_media.reindex(ideol.index)

# 3) Bar-plot
plt.figure(figsize=(10,6))
chars_pag_media.plot(kind='bar', color='skyblue')
plt.title('Longitud Media Relativa de los programas ordenados por puntuación ideológica')
plt.xlabel('Partido')
plt.ylabel('Número de caracteres/página')
plt.grid(True, axis='y')
plt.tight_layout()
plt.show()